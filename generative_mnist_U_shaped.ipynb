{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gpu\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_LEVELS = 4\n",
    "tau = 1.\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: x>0.5,\n",
    "        lambda x: x.float(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "bsize = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=True, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=False, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_binary_concrete(inputs, temperature = tau):\n",
    "    U = Variable(torch.rand(inputs.shape), requires_grad=False)\n",
    "    if cuda:\n",
    "        U = U.cuda()\n",
    "    return F.sigmoid((U.log() - (1-U).log() + inputs)/temperature)\n",
    "\n",
    "def hard_sample_binary_concrete(inputs):\n",
    "    y = sample_binary_concrete(inputs)\n",
    "    y_hard = torch.round(y)\n",
    "    return (y_hard - y).detach() + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "4\n",
      "torch.Size([100, 8])\n",
      "torch.Size([100, 4])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_levels=NUM_LEVELS):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(784, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256, 64),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "        \n",
    "        # TODO try to make tunnels smaller as we go higher.\n",
    "#         self.tunnels = nn.ModuleList([nn.Linear(128,128) for _ in range(num_levels-1)])\n",
    "        # XXX temp try\n",
    "        self.tunnels = nn.ModuleList([\n",
    "            nn.Linear(128,64),\n",
    "            nn.Linear(64,32),\n",
    "            nn.Linear(32,16),\n",
    "        ])\n",
    "#         self.tunnels = nn.ModuleList([\n",
    "#             nn.Linear(64,32),\n",
    "#             nn.Linear(32,16),\n",
    "#             nn.Linear(16,8),\n",
    "#         ])\n",
    "        \n",
    "        fc_output_shape = self.fc[-2].weight.shape[0]\n",
    "        coding_inp_sizes = [fc_output_shape]\n",
    "        for i in range(len(self.tunnels)):\n",
    "            coding_inp_sizes.append(self.tunnels[i].weight.shape[0])\n",
    "        \n",
    "        self.codings = nn.ModuleList([\n",
    "            nn.Linear(\n",
    "                coding_inp_sizes[_], 2**(num_levels-_-1) \n",
    "            ) for _ in range(num_levels)])  \n",
    "        print(self.tunnels)\n",
    "        print(self.codings)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        sofar = self.fc(x)\n",
    "        posteriors = [self.codings[0](sofar)]\n",
    "        for _ in range(len(self.tunnels)):\n",
    "            sofar = self.relu(self.tunnels[_](sofar))\n",
    "            posteriors.append(self.codings[_+1](sofar))\n",
    "        return posteriors\n",
    "    \n",
    "e = Encoder()\n",
    "_, (data, _) = next(enumerate(train_loader))\n",
    "data = data.view(-1, 784)\n",
    "data = Variable(data, requires_grad=False)\n",
    "if cuda:\n",
    "    data = data.cuda()\n",
    "    e.cuda()\n",
    "post = e(data)\n",
    "print(len(post))\n",
    "print(post[0].shape)\n",
    "print(post[1].shape)\n",
    "print(post[2].shape)\n",
    "print(post[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_levels=NUM_LEVELS):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(2**(num_levels-1), 64)\n",
    "        self.fc2 = nn.Linear(64,256)\n",
    "        self.fc3 = nn.Linear(256,784)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, code):\n",
    "        h = self.relu(self.fc1(code))\n",
    "        h = self.relu(self.fc2(h))\n",
    "        y = self.sigmoid(self.fc3(h))\n",
    "        return y\n",
    "    \n",
    "d = Decoder()\n",
    "if cuda:\n",
    "    d.cuda()\n",
    "d(post[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (2): Linear(in_features=1, out_features=2, bias=True)\n",
      ")\n",
      "4\n",
      "torch.Size([100, 1])\n",
      "torch.Size([100, 2])\n",
      "torch.Size([100, 4])\n",
      "torch.Size([100, 8])\n"
     ]
    }
   ],
   "source": [
    "class Hierarchy(nn.Module):\n",
    "    def __init__(self, num_levels=NUM_LEVELS):\n",
    "        super(Hierarchy, self).__init__()\n",
    "        self.root_dist = nn.Parameter(torch.Tensor([0.5]))\n",
    "        self.downwards = nn.ModuleList([nn.Linear(2**(_-1),2**_) for _ in range(num_levels-1, 0, -1)])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, codes):\n",
    "        n_codes = len(codes)\n",
    "        cond_priors = [self.sigmoid(self.root_dist.expand(codes[n_codes-1].size()[0],1))]\n",
    "        # codes are 8, 4, 2, 1, so are downwards\n",
    "        for i in range(n_codes-1, 0, -1):\n",
    "            cond_priors.append(self.sigmoid(self.downwards[i-1](codes[i])))\n",
    "        return cond_priors\n",
    "\n",
    "h = Hierarchy()\n",
    "if h:\n",
    "    h.cuda()\n",
    "print(h.downwards)\n",
    "codes = [hard_sample_binary_concrete(posterior) for posterior in post]\n",
    "prior = h(codes)\n",
    "print(len(prior))\n",
    "print(prior[0].shape)\n",
    "print(prior[1].shape)\n",
    "print(prior[2].shape)\n",
    "print(prior[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_levels=NUM_LEVELS):\n",
    "        super(Model, self).__init__()\n",
    "        self.E = Encoder(num_levels)\n",
    "        self.D = Decoder(num_levels)\n",
    "        self.H = Hierarchy(num_levels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        posteriors = self.E(x)\n",
    "        codes = [hard_sample_binary_concrete(posterior) for posterior in posteriors]\n",
    "        cond_priors = self.H(codes)\n",
    "        y = self.D(codes[0])\n",
    "        cond_priors.reverse() # to make it the same shape as posteriors\n",
    "        return cond_priors, posteriors, y\n",
    "        \n",
    "    def generate(self):\n",
    "        # TODO\n",
    "        pass\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), 'generative_mnist_U_shaped.save')\n",
    "        \n",
    "    def load(self):\n",
    "        self.load_state_dict(torch.load('generative_mnist_U_shaped.save'))\n",
    "    \n",
    "m = Model()\n",
    "_, (data, _) = next(enumerate(train_loader))\n",
    "data = data.view(-1, 784)\n",
    "data = Variable(data, requires_grad=False)\n",
    "if cuda:\n",
    "    m.cuda()\n",
    "    data = data.cuda()\n",
    "cond_priors, posteriors, y = m(data)\n",
    "# assert they're of the same shape order, to prevent stupid mistake in KLD\n",
    "assert cond_priors[0].shape == posteriors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLD(cond_priors, posteriors):\n",
    "    cond_priors = torch.cat(cond_priors, dim = -1)\n",
    "    posteriors = torch.cat(posteriors, dim = -1) \n",
    "    posteriors = F.sigmoid(posteriors)\n",
    "    const = 1E-20\n",
    "    kld_per_latent = posteriors * ((posteriors+const).log() - (cond_priors+const).log()) + \\\n",
    "                    (1-posteriors) * ((1-posteriors+const).log() - (1-cond_priors+const).log())\n",
    "    return kld_per_latent.sum(dim=-1).mean()\n",
    "\n",
    "def recon_loss(y, true_y):\n",
    "    return F.binary_cross_entropy(y, true_y, size_average=False)/y.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (2): Linear(in_features=32, out_features=16, bias=True)\n",
      ")\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=128, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
      "  (3): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "best_loss = 9999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global best_loss\n",
    "    train_loss = 0\n",
    "    kld_loss = 0\n",
    "    rec_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = Variable(data, requires_grad=False)\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "        x = data.view(-1,784)\n",
    "        \n",
    "        cond_priors, posteriors, y = model(x)\n",
    "        \n",
    "        kld = KLD(cond_priors, posteriors)\n",
    "        rec = recon_loss(y, x)\n",
    "        loss = kld + rec\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.data[0]\n",
    "        kld_loss += kld.data[0]\n",
    "        rec_loss += rec.data[0]\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tKLD: {:.6f}\\trecon_loss:{:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                batch_idx*100/ len(train_loader),\n",
    "                loss.data[0], kld.data[0], rec.data[0]))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}\\tKLD: {:.6f}\\trecon_loss:{:.6f}'.format(\n",
    "          epoch+1, train_loss / len(train_loader), kld_loss/ len(train_loader), rec_loss/ len(train_loader)))\n",
    "    \n",
    "    cur_loss = train_loss / len(train_loader)\n",
    "    if cur_loss < best_loss:\n",
    "        model.save()\n",
    "        best_loss = cur_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 546.292236\tKLD: 0.525637\trecon_loss:545.766602\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 210.676529\tKLD: 4.039052\trecon_loss:206.637482\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 208.550690\tKLD: 2.499348\trecon_loss:206.051346\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 210.839233\tKLD: 1.555880\trecon_loss:209.283356\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "epochs = 25\n",
    "for epoch in range(1,epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading best model with loss {}\".format(best_loss))\n",
    "model.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show everything that could be recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def debug(start=0, end=32):\n",
    "    NN = 2**(NUM_LEVELS-1) # `end` is up to NN\n",
    "    f, axes = plt.subplots(1, end-start, sharey=True, figsize=(25,1))\n",
    "    k = 0\n",
    "    kk = 0\n",
    "\n",
    "    def make_variable(value):\n",
    "        t = torch.Tensor([value]).type(torch.FloatTensor)\n",
    "        t = t.unsqueeze(0)\n",
    "        v = Variable(t)\n",
    "        if cuda:\n",
    "            v = v.cuda()\n",
    "        return v\n",
    "\n",
    "    for path in product(range(2), repeat=NN):\n",
    "        if k >= start and k < end:\n",
    "            decoder_hidden = make_variable(path)\n",
    "            w = model.D(decoder_hidden)\n",
    "            ax = axes[kk]\n",
    "            ax.axis('off')\n",
    "            #ax.set_title(path)\n",
    "            ax.imshow(w.view(28,28).cpu().data.numpy())\n",
    "            kk += 1\n",
    "        k += 1\n",
    "    plt.show()\n",
    "    \n",
    "for i in range(int(2**(2**(NUM_LEVELS-1))/32)):\n",
    "    debug(start=i*32, end=(i+1)*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image():\n",
    "    sofar = hard_sample_binary_concrete(model.H.root_dist)\n",
    "    #print(sofar)\n",
    "    for i in range(len(model.H.downwards)-1, -1, -1):\n",
    "        sofar = model.H.downwards[i](sofar)\n",
    "        sofar = hard_sample_binary_concrete(sofar)\n",
    "        #print(sofar)\n",
    "    generated = model.D(sofar)\n",
    "    return generated\n",
    "\n",
    "generated = generate_image()\n",
    "generated_img = generated.view(28,28).data.cpu()\n",
    "plt.imshow(generated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_fixed_first_level(value):\n",
    "    sofar = value#hard_sample_binary_concrete(model.H.root_dist)\n",
    "    #print(sofar)\n",
    "    for i in range(len(model.H.downwards)-1, -1, -1):\n",
    "        sofar = model.H.downwards[i](sofar)\n",
    "        sofar = hard_sample_binary_concrete(sofar)\n",
    "        #print(sofar)\n",
    "    generated = model.D(sofar)\n",
    "    return generated\n",
    "\n",
    "f, axes = plt.subplots(1, 32, sharey=True, figsize=(25,1))\n",
    "for i in range(32):\n",
    "    value = Variable(torch.Tensor([0]))\n",
    "    if cuda:\n",
    "        value = value.cuda()\n",
    "    generated = generate_image_fixed_first_level(value)\n",
    "    generated_img = generated.view(28,28).data.cpu()\n",
    "    ax = axes[i]\n",
    "    ax.imshow(generated_img)\n",
    "plt.show()\n",
    "\n",
    "f, axes = plt.subplots(1, 32, sharey=True, figsize=(25,1))\n",
    "for i in range(32):\n",
    "    value = Variable(torch.Tensor([1]))\n",
    "    if cuda:\n",
    "        value = value.cuda()\n",
    "    generated = generate_image_fixed_first_level(value)\n",
    "    generated_img = generated.view(28,28).data.cpu()\n",
    "    ax = axes[i]\n",
    "    ax.imshow(generated_img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
