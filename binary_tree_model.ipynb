{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: x>0.5,\n",
    "        lambda x: x.float(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "bsize = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=True, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=False, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N = 16\n",
    "tau = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_binary_concrete(inputs, temperature = tau):\n",
    "    U = Variable(torch.rand(inputs.shape), requires_grad=False)\n",
    "    return F.sigmoid((U.log() - (1-U).log() + inputs)/temperature)\n",
    "        \n",
    "def hard_sample_binary_concrete(inputs):\n",
    "    y = sample_binary_concrete(inputs)\n",
    "    y_hard = torch.round(y)\n",
    "    return (y_hard - y).detach() + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.relu(self.fc1(x))\n",
    "        h = self.relu(self.fc2(h))\n",
    "        h = self.relu(self.fc3(h))\n",
    "        l = self.relu(self.fc4(h))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tunnel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tunnel, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.relu(self.fc1(x))\n",
    "        l = self.relu(self.fc2(l))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.fc1(x)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(outputs):\n",
    "    outputs = [F.sigmoid(out) for out in outputs]\n",
    "    return torch.cat(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecDecoderHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecDecoderHead, self).__init__()\n",
    "        self.fc = nn.Linear(1, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc(x))\n",
    "    \n",
    "class RecDecoderBody(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecDecoderBody, self).__init__()\n",
    "        self.fc = nn.Linear(65, 64) # prev state + hidden\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc(x))\n",
    "    \n",
    "class RecDecoderTail(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(RecDecoderTail, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)      \n",
    "        self.fc3 = nn.Linear(512, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        h3 = self.fc3(h2)\n",
    "        o = h3.view(-1,784)\n",
    "        return F.sigmoid(o)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = Encoder()\n",
    "O0 = Output()\n",
    "Os = [Output() for _ in range(N-1)]\n",
    "Ts = [Tunnel() for _ in range(N-1)]\n",
    "T1s = [Tunnel() for _ in range(N-1)]\n",
    "Gs = [Gate() for _ in range(N-1)]\n",
    "D_head = RecDecoderHead()\n",
    "D_body = [RecDecoderBody() for _ in range(N-1)]\n",
    "D_tail = RecDecoderTail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    E.cuda()\n",
    "    O0.cuda()\n",
    "    [e.cuda() for e in Os]\n",
    "    [e.cuda() for e in Ts]\n",
    "    [e.cuda() for e in T1s]\n",
    "    [e.cuda() for e in Gs]\n",
    "    D_head.cuda()\n",
    "    [e.cuda() for e in D_body]\n",
    "    D_tail.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('mkdir -p checkpoint_bdt_vae')\n",
    "PATH = 'checkpoint_bdt_vae/'\n",
    "def save_model():\n",
    "    torch.save(E.state_dict(), \"{}_E\".format(PATH))\n",
    "    torch.save(O0.state_dict(), \"{}_O0\".format(PATH))\n",
    "    \n",
    "    def save_list(models, name):\n",
    "        for i in range(len(models)):\n",
    "            torch.save(models[i].state_dict(), \"{}_{}_{}\".format(PATH, name, i))\n",
    "    \n",
    "    save_list(Os, 'Os')\n",
    "    save_list(Ts, 'Ts')\n",
    "    save_list(T1s, 'T1s')\n",
    "    save_list(Gs, 'Gs')\n",
    "    \n",
    "    torch.save(D_head.state_dict(), \"{}_D_head\".format(PATH))\n",
    "    save_list(D_body, 'D_body')\n",
    "    torch.save(D_tail.state_dict(), \"{}_D_tail\".format(PATH))\n",
    "    \n",
    "#save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# objective\n",
    "def loss_function(recon_x, x, q):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)/bsize # TODO eddie: changed from size_average=False\n",
    "    # sample KLDs\n",
    "    KLD = torch.sum(q*(torch.log(q+1E-20)-np.log(0.5))+(1-q)*(torch.log(1-q+1E-20)-np.log(0.5)))/bsize\n",
    "\n",
    "    return BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {'params':E.parameters()},\n",
    "    {'params':O0.parameters()} \n",
    "] + [{'params':o.parameters()} for o in Os] \\\n",
    "  + [{'params':t.parameters()} for t in Ts] \\\n",
    "  + [{'params': g.parameters()} for g in Gs] \\\n",
    "  + [{'params': D_head.parameters()}, {'params': D_tail.parameters()}] \\\n",
    "  + [{'params': d_body.parameters()} for d_body in D_body]\n",
    "optimizer = optim.Adam(params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_sofar = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-d719d90aef42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-d719d90aef42>\u001b[0m in \u001b[0;36mdebug\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mkk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1177\u001b[0m     axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[1;32m   1178\u001b[0m                        \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot_kw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                        gridspec_kw=gridspec_kw)\n\u001b[0m\u001b[1;32m   1180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msubplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 \u001b[0msubplot_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sharex\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_with\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msharex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0msubplot_kw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sharey\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshared_with\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msharey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                 \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0;31m# turn off redundant tick labeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Illegal argument(s) to subplot: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumRows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumCols\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             self.get_subplotspec().get_position(self.figure,\n\u001b[0;32m--> 114\u001b[0;31m                                                 return_all=True)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_first_col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36mget_position\u001b[0;34m(self, fig, return_all)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mfigBottoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigTops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigLefts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigRights\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0mgridspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grid_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mrowNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolNum\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36mget_grid_positions\u001b[0;34m(self, fig)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mfigTops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcellHs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrowNum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrowNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mfigBottoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcellHs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrowNum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrowNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mfigLefts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcellWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolNum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mfigRights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcellWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolNum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/matplotlib/gridspec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mfigTops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcellHs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrowNum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrowNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mfigBottoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcellHs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrowNum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrowNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mfigLefts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcellWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolNum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mfigRights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcellWs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolNum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolNum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def debug():\n",
    "    f, axes = plt.subplots(1, 2**N, sharey=True, figsize=(25,1))\n",
    "    kk = 0\n",
    "\n",
    "    def make_variable(value):\n",
    "        t = torch.Tensor([value]).type(torch.FloatTensor)\n",
    "        t = t.unsqueeze(0)\n",
    "        v = Variable(t)\n",
    "        if CUDA:\n",
    "            v = v.cuda()\n",
    "        return v\n",
    "\n",
    "    for path in product(range(2), repeat=N):\n",
    "        decoder_hidden = D_head(make_variable(path[0]))\n",
    "        for n in range(1, len(path)):\n",
    "            decoder_hidden_inp = torch.cat([decoder_hidden, make_variable(path[n])], -1)\n",
    "            decoder_hidden = D_body[n-1](decoder_hidden_inp)\n",
    "        w = D_tail(decoder_hidden)\n",
    "\n",
    "        ax = axes[kk]\n",
    "        ax.set_title(path)\n",
    "        ax.imshow(w.view(28,28).cpu().data.numpy())\n",
    "        kk += 1\n",
    "    plt.show()\n",
    "    \n",
    "debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global best_loss_sofar\n",
    "    train_bce_loss = 0\n",
    "    train_kld_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        x = data.view(-1,784)\n",
    "        optimizer.zero_grad()\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "        samples = [hard_sample_binary_concrete(outputs[-1])]\n",
    "        decoder_hidden = D_head(samples[-1])\n",
    "        # TODO unroll this may make it faster\n",
    "        for n in range(len(Ts)):\n",
    "            sofar = Ts[n](sofar)*Gs[n](decoder_hidden)\n",
    "            outputs.append(Os[n](sofar))\n",
    "            samples.append(hard_sample_binary_concrete(outputs[-1]))\n",
    "            \n",
    "            decoder_hidden_inp = torch.cat([decoder_hidden, samples[-1]], -1)\n",
    "            decoder_hidden = D_body[n](decoder_hidden_inp)\n",
    "            \n",
    "        recon_x = D_tail(decoder_hidden)\n",
    "     \n",
    "        bits = torch.cat(samples, dim=1) # for debugging only\n",
    "        q = prob(outputs)\n",
    "\n",
    "        BCE, KLD = loss_function(recon_x, x, q)\n",
    "        loss = BCE + KLD\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_bce_loss += BCE.data[0]\n",
    "        train_kld_loss += KLD.data[0]\n",
    "        \n",
    "#         if batch_idx % 200 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBCE loss: {:.6f}\\tKLD loss: {:.6f}'.format(\n",
    "#                 epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader),\n",
    "#                 BCE.data[0] / len(data),\n",
    "#                 KLD.data[0] / len(data)))\n",
    "            \n",
    "    avg_bce_loss = train_bce_loss / len(train_loader)\n",
    "    avg_kld_loss = train_kld_loss / len(train_loader)\n",
    "    avg_loss = avg_bce_loss + avg_kld_loss\n",
    "    print('====> Epoch: {} Average BCE loss: {:.4f}, Average KLD loss: {:.4f}, Total: {:.4f}'.format(\n",
    "          epoch+1, avg_bce_loss, avg_kld_loss, avg_loss))\n",
    "    \n",
    "    if avg_loss < best_loss_sofar:\n",
    "        print(\"Loss {} is better than previous best {}, saving model\".format(avg_loss, best_loss_sofar))\n",
    "        save_model()\n",
    "        best_loss_sofar = avg_loss\n",
    "#         debug()\n",
    "        \n",
    "    return q, bits # debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(42)\n",
    "for epoch in range(10):\n",
    "    q, bits = train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_bce_loss = 0\n",
    "    test_kld_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(test_loader):\n",
    "        data = Variable(data)\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        x = data.view(-1,784)\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "        samples = [hard_sample_binary_concrete(outputs[-1])]\n",
    "        decoder_hidden = D_head(samples[-1])\n",
    "        # TODO unroll this may make it faster\n",
    "        for n in range(len(Ts)):\n",
    "            sofar = Ts[n](sofar)*Gs[n](decoder_hidden)\n",
    "            outputs.append(Os[n](sofar))\n",
    "            samples.append(hard_sample_binary_concrete(outputs[-1]))\n",
    "            \n",
    "            decoder_hidden_inp = torch.cat([decoder_hidden, samples[-1]], -1)\n",
    "            decoder_hidden = D_body[n](decoder_hidden_inp)\n",
    "            \n",
    "        recon_x = D_tail(decoder_hidden)\n",
    "     \n",
    "        bits = torch.cat(samples, dim=1) # for debugging only\n",
    "        q = prob(outputs)\n",
    "\n",
    "        BCE, KLD = loss_function(recon_x, x, q)\n",
    "        loss = BCE + KLD\n",
    "        \n",
    "        test_bce_loss += BCE.data[0]\n",
    "        test_kld_loss += KLD.data[0]\n",
    "            \n",
    "    avg_bce_loss = test_bce_loss / len(test_loader)\n",
    "    avg_kld_loss = test_kld_loss / len(test_loader)\n",
    "    avg_loss = avg_bce_loss + avg_kld_loss\n",
    "    print('Average BCE loss: {:.4f}, Average KLD loss: {:.4f}, Total: {:.4f}'.format(\n",
    "        avg_bce_loss, avg_kld_loss, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BCE loss: 180.4100, Average KLD loss: 3.1053, Total: 183.5152\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
