{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: x>0.5,\n",
    "        lambda x: x.float(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "bsize = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=True, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=False, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N = 5\n",
    "k = 2\n",
    "tau = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: UPDATE TO BINARY GUMBEL (REFER TO L0-REG PAPER)\n",
    "# gumbel-softmax\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    return -Variable(torch.log(-torch.log(U + eps) + eps))\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    return (y_hard - y).detach() + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        l = self.relu(self.fc2(h1))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tunnel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tunnel, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.relu(self.fc1(x))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        l = h1.view(-1,2)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob(outputs):\n",
    "    outputs = [F.softmax(out, dim=-1) for out in outputs]\n",
    "    return torch.cat(outputs, dim=1).view(-1, N, k)\n",
    "\n",
    "def sample(l):\n",
    "    gs = gumbel_softmax(l, tau)\n",
    "    return gs.narrow(1,0,1)\n",
    "\n",
    "# def signal(outputs):\n",
    "#     outputs = [sample(out).narrow(1,0,1) for out in outputs]\n",
    "#     return torch.cat(outputs, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(N, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)      \n",
    "        self.fc4 = nn.Linear(512, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        h3 = self.relu(self.fc3(h2))\n",
    "        h4 = self.fc4(h3)\n",
    "        o = h4.view(-1,784)\n",
    "        return F.sigmoid(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = Encoder()\n",
    "D = Decoder()\n",
    "O0 = Output()\n",
    "Os = [Output() for _ in range(N-1)]\n",
    "Ts = [Tunnel() for _ in range(N-1)]\n",
    "T1s = [Tunnel() for _ in range(N-1)]\n",
    "Gs = [Gate() for _ in range(N-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "    \n",
    "log_two_pow_n = Variable(torch.Tensor([2**N]).log(), requires_grad=False)\n",
    "# TODO cuda it\n",
    "\n",
    "def get_dependent_prior_loss(x):\n",
    "    acc = Variable(torch.zeros(2**N))\n",
    "    acc_i = 0\n",
    "    for _path in product(range(2), repeat=N):\n",
    "        path = Variable(torch.Tensor(_path), requires_grad=False)\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "\n",
    "        for n in range(len(Ts)):\n",
    "            sofar = Ts[n](sofar)*Gs[n](path[n]) # Kind of doing \"teacher forcing\"\n",
    "            outputs.append(Os[n](sofar))\n",
    "\n",
    "        q = prob(outputs)\n",
    "\n",
    "        # TODO is there a better way to select along a dim?\n",
    "        _idx = path.data\n",
    "        _idx = torch.cat([(1-_idx).unsqueeze(1), _idx.unsqueeze(1)], 1)\n",
    "        _idx = Variable(_idx, requires_grad=False)\n",
    "        _idx = _idx.expand(x.shape[0], _idx.shape[0], _idx.shape[1])\n",
    "        _idx = _idx.byte()\n",
    "\n",
    "        # Debug\n",
    "    #     print(q)\n",
    "    #     print(path)\n",
    "    #     print(q.masked_select(_idx))\n",
    "\n",
    "        probs = q.masked_select(_idx)\n",
    "\n",
    "    #     # less numerically stable\n",
    "    #     q = probs.log().sum().exp()\n",
    "    #     kl_term = q.mul(q.log() + log_two_pow_n) # TODO get rid of numpy\n",
    "\n",
    "        # more numerically stable\n",
    "        log_q = (probs+1e-20).log().sum() # TODO hacky to fix log becoming inf\n",
    "        kl_term = log_q.exp().mul(log_two_pow_n + log_q)\n",
    "\n",
    "        acc[acc_i] = kl_term\n",
    "        acc_i += 1\n",
    "\n",
    "    return acc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-04 *\n",
       "  9.1317\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING\n",
    "# Hacky! Getting a data point.\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    x = data.view(-1,784)\n",
    "    x = x[0:1]\n",
    "    break\n",
    "    \n",
    "get_dependent_prior_loss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# objective\n",
    "def loss_function(recon_x, x, q):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "    \n",
    "    # Assuming independent latent\n",
    "#     KLD = torch.sum(q*(torch.log(q+1E-20)-np.log(1/k)))\n",
    "\n",
    "    # Assuming dependent latent\n",
    "    KLD = get_dependent_prior_loss(x)\n",
    "    \n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 542.743320\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 206.597891\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 201.881309\n",
      "====> Epoch: 1 Average loss: 234.3373\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 189.357246\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 201.327383\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 192.332754\n",
      "====> Epoch: 2 Average loss: 428.5909\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 194.700234\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 190.167539\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 190.780391\n",
      "====> Epoch: 3 Average loss: 620.4394\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 190.616484\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 181.327188\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 178.462871\n",
      "====> Epoch: 4 Average loss: 806.4095\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 186.105566\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 175.810547\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 170.164199\n",
      "====> Epoch: 5 Average loss: 985.7366\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 173.207617\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 175.329512\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 177.851250\n",
      "====> Epoch: 6 Average loss: 1163.1487\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 174.981582\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 174.487480\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 172.242012\n",
      "====> Epoch: 7 Average loss: 1339.3045\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 175.258633\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 173.536055\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 173.992266\n",
      "====> Epoch: 8 Average loss: 1514.6321\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 169.043711\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 180.753262\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 170.471973\n",
      "====> Epoch: 9 Average loss: 1689.4568\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 173.140977\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 179.127168\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 171.051699\n",
      "====> Epoch: 10 Average loss: 1863.8565\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "epochs = 10\n",
    "\n",
    "params = [\n",
    "    {'params':E.parameters()},\n",
    "    {'params':D.parameters()},\n",
    "    {'params':O0.parameters()},\n",
    "] + [{'params':o.parameters()} for o in Os] \\\n",
    "  + [{'params':t.parameters()} for t in Ts] + [{'params': g.parameters()} for g in Gs]\n",
    "optimizer = optim.Adam(params, lr=1e-4)\n",
    "\n",
    "train_loss = 0\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        x = data.view(-1,784)\n",
    "        optimizer.zero_grad()\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "        samples = [sample(outputs[-1])]\n",
    "        for n in range(len(Ts)):\n",
    "            sofar = Ts[n](sofar)*Gs[n](samples[-1])\n",
    "            outputs.append(Os[n](sofar))\n",
    "            samples.append(sample(outputs[-1]))\n",
    "#         for n in range(len(T1s)):\n",
    "#             soforth = T1s[n](outputs[n])\n",
    "\n",
    "#         recon_x = D(soforth)\n",
    "            \n",
    "        bits = torch.cat(samples, dim=1)\n",
    "        q = prob(outputs)\n",
    "        recon_x = D(bits)\n",
    "        loss = loss_function(recon_x, x, q)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0] / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch+1, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  1.0000e+00  7.3847e-07\n",
       "  9.9950e-01  5.0239e-04\n",
       "  3.9478e-01  6.0522e-01\n",
       "  4.2249e-03  9.9578e-01\n",
       "  4.4656e-05  9.9996e-01\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  6.4015e-05  9.9994e-01\n",
       "  8.6019e-05  9.9991e-01\n",
       "  1.5498e-03  9.9845e-01\n",
       "  9.9996e-01  3.8534e-05\n",
       "  1.6997e-11  1.0000e+00\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  1.2252e-05  9.9999e-01\n",
       "  8.1277e-01  1.8723e-01\n",
       "  6.8557e-01  3.1443e-01\n",
       "  1.0000e+00  2.6835e-06\n",
       "  7.6302e-17  1.0000e+00\n",
       "... \n",
       "\n",
       "(97 ,.,.) = \n",
       "  1.6409e-01  8.3591e-01\n",
       "  9.9955e-01  4.5477e-04\n",
       "  4.6864e-01  5.3136e-01\n",
       "  8.5946e-01  1.4054e-01\n",
       "  2.6085e-16  1.0000e+00\n",
       "\n",
       "(98 ,.,.) = \n",
       "  9.2785e-08  1.0000e+00\n",
       "  7.6906e-03  9.9231e-01\n",
       "  9.9837e-01  1.6273e-03\n",
       "  9.0698e-01  9.3019e-02\n",
       "  5.5783e-13  1.0000e+00\n",
       "\n",
       "(99 ,.,.) = \n",
       "  2.0036e-03  9.9800e-01\n",
       "  1.3056e-04  9.9987e-01\n",
       "  3.4141e-04  9.9966e-01\n",
       "  9.9995e-01  5.0259e-05\n",
       "  3.0345e-06  1.0000e+00\n",
       "[torch.FloatTensor of size 100x5x2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    1     1     1     0     0\n",
       "    0     0     0     1     0\n",
       "    0     1     1     1     0\n",
       "    0     0     1     0     0\n",
       "    1     0     1     1     0\n",
       "    1     1     0     0     0\n",
       "    0     1     1     1     0\n",
       "    1     1     0     1     0\n",
       "    1     1     0     1     0\n",
       "    1     1     1     1     0\n",
       "    1     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    0     1     1     0     0\n",
       "    0     0     0     1     0\n",
       "    1     1     1     0     0\n",
       "    1     1     0     0     0\n",
       "    1     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    1     1     1     1     0\n",
       "    1     0     1     0     0\n",
       "    1     0     0     1     0\n",
       "    1     1     0     0     0\n",
       "    1     0     1     0     0\n",
       "    0     1     0     1     0\n",
       "    0     0     0     0     0\n",
       "    1     0     0     1     0\n",
       "    1     0     0     0     0\n",
       "    0     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    0     1     1     0     0\n",
       "    0     0     0     1     0\n",
       "    0     1     0     1     0\n",
       "    0     1     1     1     0\n",
       "    1     0     1     0     0\n",
       "    0     0     0     1     0\n",
       "    1     1     0     1     0\n",
       "    0     1     1     0     0\n",
       "    1     1     0     1     0\n",
       "    1     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    1     1     0     1     0\n",
       "    1     0     1     1     0\n",
       "    0     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    1     1     0     1     0\n",
       "    1     0     1     1     0\n",
       "    1     1     1     0     0\n",
       "    0     1     1     1     0\n",
       "    1     1     1     0     0\n",
       "    0     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    0     1     1     1     0\n",
       "    1     0     1     0     0\n",
       "    1     1     1     0     0\n",
       "    1     1     0     0     0\n",
       "    1     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    1     0     1     0     0\n",
       "    0     1     1     1     0\n",
       "    0     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    1     0     1     1     0\n",
       "    1     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    1     0     1     1     0\n",
       "    0     0     0     1     0\n",
       "    1     1     0     0     0\n",
       "    1     0     0     1     0\n",
       "    1     0     1     0     0\n",
       "    1     1     1     1     0\n",
       "    0     1     0     1     0\n",
       "    0     1     1     0     0\n",
       "    0     0     0     1     0\n",
       "    1     1     0     1     0\n",
       "    1     0     0     1     0\n",
       "    0     0     1     1     0\n",
       "    1     0     1     1     0\n",
       "    0     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    1     1     0     0     0\n",
       "    0     1     0     1     0\n",
       "    0     0     1     0     0\n",
       "    1     0     0     1     0\n",
       "    1     0     0     1     0\n",
       "    1     1     0     0     0\n",
       "    1     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    0     0     0     1     0\n",
       "    1     0     1     0     0\n",
       "    1     0     0     1     0\n",
       "    1     1     0     0     0\n",
       "    1     1     1     1     0\n",
       "    0     1     0     1     0\n",
       "    0     0     1     1     0\n",
       "    0     0     0     1     0\n",
       "[torch.FloatTensor of size 100x5]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = dset.MNIST('data', train=False, download=True, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.array([[0., 0., 0., 0.]])\n",
    "# w = D.forward(Variable(torch.from_numpy(a).type(torch.FloatTensor)))\n",
    "# plt.imshow(w.view(28,28).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAABlCAYAAADAmYakAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzJIkSZIkSZKkVv4Pe+4aCX+KFCgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b473860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import product\n",
    "f, axes = plt.subplots(1, 2**N, sharey=True, figsize=(25,1))\n",
    "kk = 0\n",
    "for path in product(range(2), repeat=N):\n",
    "    w = D.forward(Variable(torch.Tensor(path).type(torch.FloatTensor)))\n",
    "    ax = axes[k]\n",
    "    ax.set_title(path)\n",
    "    ax.imshow(w.view(28,28).data.numpy())\n",
    "    kk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
