{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: x>0.5,\n",
    "        lambda x: x.float(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "bsize = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=True, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=False, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N = 5\n",
    "k = 2\n",
    "tau = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: UPDATE TO BINARY GUMBEL (REFER TO L0-REG PAPER)\n",
    "# gumbel-softmax\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    return -Variable(torch.log(-torch.log(U + eps) + eps))\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    return (y_hard - y).detach() + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        l = self.relu(self.fc2(h1))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tunnel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tunnel, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.relu(self.fc1(x))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        l = h1.view(-1,2)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob(outputs):\n",
    "    outputs = [F.softmax(out, dim=-1) for out in outputs]\n",
    "    return torch.cat(outputs, dim=1).view(-1, N, k)\n",
    "\n",
    "def sample(l):\n",
    "    gs = gumbel_softmax(l, tau)\n",
    "    return gs.narrow(1,0,1)\n",
    "\n",
    "# def signal(outputs):\n",
    "#     outputs = [sample(out).narrow(1,0,1) for out in outputs]\n",
    "#     return torch.cat(outputs, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(N, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)      \n",
    "        self.fc4 = nn.Linear(512, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        h3 = self.relu(self.fc3(h2))\n",
    "        h4 = self.fc4(h3)\n",
    "        o = h4.view(-1,784)\n",
    "        return F.sigmoid(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E = Encoder()\n",
    "D = Decoder()\n",
    "O0 = Output()\n",
    "Os = [Output() for _ in range(N-1)]\n",
    "Ts = [Tunnel() for _ in range(N-1)]\n",
    "T1s = [Tunnel() for _ in range(N-1)]\n",
    "Gs = [Gate() for _ in range(N-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "    \n",
    "log_two_pow_n = Variable(torch.Tensor([2**N]).log(), requires_grad=False)\n",
    "# TODO cuda it\n",
    "\n",
    "def get_dependent_prior_loss(x):\n",
    "    acc = Variable(torch.zeros(2**N))\n",
    "    acc_i = 0\n",
    "    for _path in product(range(2), repeat=N):\n",
    "        path = Variable(torch.Tensor(_path), requires_grad=False)\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "\n",
    "        for n in range(len(Ts)):\n",
    "            sofar = Ts[n](sofar)*Gs[n](path[n]) # Kind of doing \"teacher forcing\"\n",
    "            outputs.append(Os[n](sofar))\n",
    "\n",
    "        q = prob(outputs)\n",
    "\n",
    "        _idx = path.data\n",
    "        select_mat = Variable(torch.stack([1-_idx, _idx]).t(), requires_grad=False)\n",
    "        select_mat = select_mat.expand(torch.Size([x.shape[0]]) + select_mat.shape)\n",
    "        probs = q.mul(select_mat).sum(-1) # shape (batch x N)\n",
    "        log_q = (probs+1e-20).log().sum(-1) # shape (batch)\n",
    "        kl_term = log_q.exp().mul(log_two_pow_n + log_q) # shape (batch)\n",
    "        kl_term = kl_term.mean() # shape (1) - take sum over minibatch\n",
    "\n",
    "        acc[acc_i] = kl_term\n",
    "        acc_i += 1\n",
    "\n",
    "    return acc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "1.00000e-03 *\n",
       "  1.8357\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING\n",
    "# Hacky! Getting a data point.\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    x = data.view(-1,784)\n",
    "    x = x[0:4]\n",
    "    break\n",
    "    \n",
    "get_dependent_prior_loss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# objective\n",
    "def loss_function(recon_x, x, q):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=True) # TODO eddie: changed from size_average=False\n",
    "    \n",
    "    # Assuming independent latent\n",
    "#     KLD = torch.sum(q*(torch.log(q+1E-20)-np.log(1/k)))\n",
    "\n",
    "    # Assuming dependent latent\n",
    "    KLD = get_dependent_prior_loss(x)\n",
    "    \n",
    "    return BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [\n",
    "    {'params':E.parameters()},\n",
    "    {'params':D.parameters()},\n",
    "    {'params':O0.parameters()},\n",
    "] + [{'params':o.parameters()} for o in Os] \\\n",
    "  + [{'params':t.parameters()} for t in Ts] + [{'params': g.parameters()} for g in Gs]\n",
    "optimizer = optim.Adam(params, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tBCE loss: 0.006934\tKLD loss: 0.000019\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tBCE loss: 0.002698\tKLD loss: 0.000000\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tBCE loss: 0.002706\tKLD loss: 0.000000\n",
      "====> Epoch: 1 Average BCE loss: 0.0031, Average KLD loss: 0.0000\n",
      "Train Epoch: 2 [0/60000 (0%)]\tBCE loss: 0.002599\tKLD loss: 0.000000\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tBCE loss: 0.002589\tKLD loss: 0.000000\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tBCE loss: 0.002606\tKLD loss: 0.000000\n",
      "====> Epoch: 2 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 3 [0/60000 (0%)]\tBCE loss: 0.002565\tKLD loss: 0.000000\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tBCE loss: 0.002714\tKLD loss: 0.000000\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tBCE loss: 0.002722\tKLD loss: 0.000000\n",
      "====> Epoch: 3 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 4 [0/60000 (0%)]\tBCE loss: 0.002670\tKLD loss: 0.000000\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tBCE loss: 0.002692\tKLD loss: 0.000000\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tBCE loss: 0.002655\tKLD loss: 0.000000\n",
      "====> Epoch: 4 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 5 [0/60000 (0%)]\tBCE loss: 0.002688\tKLD loss: 0.000000\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tBCE loss: 0.002676\tKLD loss: 0.000000\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tBCE loss: 0.002686\tKLD loss: 0.000000\n",
      "====> Epoch: 5 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 6 [0/60000 (0%)]\tBCE loss: 0.002595\tKLD loss: 0.000000\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tBCE loss: 0.002556\tKLD loss: 0.000000\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tBCE loss: 0.002665\tKLD loss: 0.000000\n",
      "====> Epoch: 6 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 7 [0/60000 (0%)]\tBCE loss: 0.002544\tKLD loss: 0.000000\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tBCE loss: 0.002534\tKLD loss: 0.000000\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tBCE loss: 0.002631\tKLD loss: 0.000000\n",
      "====> Epoch: 7 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 8 [0/60000 (0%)]\tBCE loss: 0.002593\tKLD loss: 0.000000\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tBCE loss: 0.002569\tKLD loss: 0.000000\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tBCE loss: 0.002657\tKLD loss: 0.000000\n",
      "====> Epoch: 8 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 9 [0/60000 (0%)]\tBCE loss: 0.002730\tKLD loss: 0.000000\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tBCE loss: 0.002631\tKLD loss: 0.000000\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tBCE loss: 0.002641\tKLD loss: 0.000000\n",
      "====> Epoch: 9 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "Train Epoch: 10 [0/60000 (0%)]\tBCE loss: 0.002742\tKLD loss: 0.000000\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tBCE loss: 0.002702\tKLD loss: 0.000000\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tBCE loss: 0.002584\tKLD loss: 0.000000\n",
      "====> Epoch: 10 Average BCE loss: 0.0026, Average KLD loss: 0.0000\n",
      "CPU times: user 1h 2min 7s, sys: 6min 9s, total: 1h 8min 16s\n",
      "Wall time: 51min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_bce_loss = 0\n",
    "    train_kld_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        x = data.view(-1,784)\n",
    "        optimizer.zero_grad()\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "        samples = [sample(outputs[-1])]\n",
    "        for n in range(len(Ts)):\n",
    "            sofar = Ts[n](sofar)*Gs[n](samples[-1])\n",
    "            outputs.append(Os[n](sofar))\n",
    "            samples.append(sample(outputs[-1]))\n",
    "        bits = torch.cat(samples, dim=1)\n",
    "        q = prob(outputs)\n",
    "        recon_x = D(bits)\n",
    "        BCE, KLD = loss_function(recon_x, x, q)\n",
    "        loss = BCE + KLD\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_bce_loss += BCE.data[0]\n",
    "        train_kld_loss += KLD.data[0]\n",
    "        \n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBCE loss: {:.6f}\\tKLD loss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                BCE.data[0] / len(data),\n",
    "                KLD.data[0] / len(data)))\n",
    "            \n",
    "    print('====> Epoch: {} Average BCE loss: {:.4f}, Average KLD loss: {:.4f}'.format(\n",
    "          epoch+1, train_bce_loss / len(train_loader.dataset), train_kld_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "... \n",
       "\n",
       "(97 ,.,.) = \n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "\n",
       "(98 ,.,.) = \n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "\n",
       "(99 ,.,.) = \n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "  0.5000  0.5000\n",
       "[torch.FloatTensor of size 100x5x2]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    1     1     1     0     1\n",
       "    0     1     0     0     0\n",
       "    0     0     0     0     0\n",
       "    1     0     1     1     0\n",
       "    0     0     1     1     0\n",
       "    0     1     0     0     0\n",
       "    1     1     0     1     1\n",
       "    1     0     0     0     0\n",
       "    0     1     1     0     1\n",
       "    1     1     1     1     0\n",
       "    0     0     1     1     0\n",
       "    1     0     1     1     0\n",
       "    0     1     1     1     0\n",
       "    0     0     1     0     1\n",
       "    0     1     1     1     1\n",
       "    1     0     1     0     1\n",
       "    1     1     0     1     1\n",
       "    0     1     1     1     1\n",
       "    1     1     0     0     0\n",
       "    0     1     1     1     1\n",
       "    0     0     1     1     0\n",
       "    0     1     0     1     1\n",
       "    1     1     0     1     0\n",
       "    0     0     0     1     0\n",
       "    0     1     1     1     0\n",
       "    0     1     0     1     1\n",
       "    0     1     0     1     1\n",
       "    0     0     1     0     1\n",
       "    1     0     1     1     0\n",
       "    0     1     1     0     1\n",
       "    1     0     0     1     0\n",
       "    1     0     0     0     0\n",
       "    1     1     0     1     1\n",
       "    0     1     1     1     1\n",
       "    0     1     1     0     1\n",
       "    0     1     0     0     1\n",
       "    1     0     0     0     0\n",
       "    0     1     0     0     0\n",
       "    1     1     1     1     1\n",
       "    0     1     1     0     0\n",
       "    0     0     1     0     1\n",
       "    0     1     0     0     1\n",
       "    0     0     1     0     0\n",
       "    1     1     1     0     0\n",
       "    0     0     0     0     1\n",
       "    1     0     1     1     1\n",
       "    1     1     0     1     1\n",
       "    1     0     0     1     0\n",
       "    0     1     0     0     1\n",
       "    0     1     0     1     0\n",
       "    0     0     1     1     0\n",
       "    0     1     0     1     1\n",
       "    0     0     1     1     0\n",
       "    1     1     1     0     0\n",
       "    1     0     0     0     1\n",
       "    1     0     1     1     0\n",
       "    1     0     0     1     0\n",
       "    1     0     1     0     1\n",
       "    1     1     0     1     1\n",
       "    0     0     1     1     0\n",
       "    0     0     1     1     0\n",
       "    1     0     0     1     0\n",
       "    0     1     1     1     1\n",
       "    1     1     1     1     1\n",
       "    0     1     0     0     0\n",
       "    1     0     0     0     1\n",
       "    0     0     1     1     0\n",
       "    0     0     1     0     1\n",
       "    1     1     1     0     0\n",
       "    1     1     1     0     1\n",
       "    1     1     0     0     0\n",
       "    1     1     1     1     1\n",
       "    0     1     0     1     0\n",
       "    1     1     0     0     0\n",
       "    0     0     1     0     0\n",
       "    0     0     0     1     1\n",
       "    0     1     0     0     0\n",
       "    0     1     1     0     1\n",
       "    0     1     0     0     0\n",
       "    1     0     1     0     0\n",
       "    0     1     0     1     1\n",
       "    0     0     0     1     0\n",
       "    1     0     1     1     0\n",
       "    1     0     0     1     1\n",
       "    0     1     1     1     0\n",
       "    0     0     0     1     0\n",
       "    0     0     0     0     1\n",
       "    1     1     0     0     1\n",
       "    0     0     0     1     0\n",
       "    1     0     0     0     0\n",
       "    1     1     1     0     1\n",
       "    0     1     0     0     1\n",
       "    1     1     0     0     0\n",
       "    1     1     0     0     0\n",
       "    0     1     1     1     1\n",
       "    0     0     1     0     1\n",
       "    1     0     1     1     0\n",
       "    0     0     0     0     0\n",
       "    1     0     1     0     0\n",
       "    0     0     1     1     1\n",
       "[torch.FloatTensor of size 100x5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = dset.MNIST('data', train=False, download=True, transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.array([[0., 0., 0., 0.]])\n",
    "# w = D.forward(Variable(torch.from_numpy(a).type(torch.FloatTensor)))\n",
    "# plt.imshow(w.view(28,28).data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAABlCAYAAADAmYakAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1186809b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import product\n",
    "f, axes = plt.subplots(1, 2**N, sharey=True, figsize=(25,1))\n",
    "kk = 0\n",
    "for path in product(range(2), repeat=N):\n",
    "    w = D.forward(Variable(torch.Tensor(path).type(torch.FloatTensor)))\n",
    "    ax = axes[k]\n",
    "    ax.set_title(path)\n",
    "    ax.imshow(w.view(28,28).data.numpy())\n",
    "    kk += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
