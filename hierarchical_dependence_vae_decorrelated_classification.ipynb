{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #transform\n",
    "# data_transforms = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         lambda x: x>0.5,\n",
    "#         lambda x: x.float(),\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "bsize = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.ToTensor()),\n",
    "    batch_size=bsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=bsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d53be7a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_test_img(ind=0):\n",
    "    batch_idx, (data, ys) = next(enumerate(test_loader))\n",
    "    test_img = Variable(data[ind:ind+1])\n",
    "    return test_img\n",
    "\n",
    "def show_test_img(ind=0):\n",
    "    plt.imshow(get_test_img(ind).view(28,28).data)\n",
    "    \n",
    "def show_img(img):\n",
    "    plt.imshow(img.view(28,28).data)\n",
    "    \n",
    "show_test_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N = 4\n",
    "k = 2\n",
    "tau = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: UPDATE TO BINARY GUMBEL (REFER TO L0-REG PAPER)\n",
    "# gumbel-softmax\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    v = Variable(torch.log(-torch.log(U + eps) + eps))\n",
    "    if CUDA:\n",
    "        v = v.cuda()\n",
    "    return -v\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    return (y_hard - y).detach() + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conv version encoder\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= x.view(-1,1,28,28)\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.fc1 = nn.Linear(784, 256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "#         self.relu = nn.ReLU()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         h1 = self.relu(self.fc1(x))\n",
    "#         l = self.relu(self.fc2(h1))\n",
    "#         return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tunnel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tunnel, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.relu(self.fc1(x))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        l = h1.view(-1,2)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob(outputs):\n",
    "    outputs = [F.softmax(out, dim=-1) for out in outputs]\n",
    "    return torch.cat(outputs, dim=1).view(-1, N, k)\n",
    "\n",
    "def sample(l):\n",
    "    gs = gumbel_softmax(l, tau)\n",
    "    return gs.narrow(1,0,1)\n",
    "\n",
    "# def signal(outputs):\n",
    "#     outputs = [sample(out).narrow(1,0,1) for out in outputs]\n",
    "#     return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecDecoderHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecDecoderHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc1(x))\n",
    "    \n",
    "class RecDecoderBody(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecDecoderBody, self).__init__()\n",
    "        self.fc = nn.Linear(129, 128) # prev state + hidden\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc(x))\n",
    "    \n",
    "class RecDecoderTail(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(RecDecoderTail, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)      \n",
    "        self.fc3 = nn.Linear(512, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        h3 = self.fc3(h2)\n",
    "        o = h3.view(-1,784)\n",
    "        return F.sigmoid(o)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.E = Encoder()\n",
    "        self.Ts = nn.ModuleList([Tunnel() for _ in range(N-1)])\n",
    "        self.Gs = nn.ModuleList([Gate() for _ in range(N-1)])\n",
    "        self.Os = nn.ModuleList([Output() for _ in range(N-1)])\n",
    "        self.O0 = Output()\n",
    "        self.D_head = RecDecoderHead()\n",
    "        self.D_body = nn.ModuleList([RecDecoderBody() for _ in range(N-1)])\n",
    "        self.Ls = nn.ModuleList([nn.Linear(_, 10) for _ in range(1,N+1)])\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sofar = self.E(x)\n",
    "        outputs = [self.O0(sofar)]\n",
    "        \n",
    "        samples = [sample(outputs[-1])]\n",
    "        decoder_hidden = self.D_head(samples[-1])\n",
    "        # TODO unroll this may make it faster\n",
    "        for n in range(len(self.Ts)):\n",
    "            sofar = self.Ts[n](sofar)*self.Gs[n](decoder_hidden)\n",
    "            outputs.append(self.Os[n](sofar))\n",
    "            samples.append(sample(outputs[-1]))\n",
    "            \n",
    "            decoder_hidden_inp = torch.cat([decoder_hidden, samples[-1]], -1)\n",
    "            decoder_hidden = self.D_body[n](decoder_hidden_inp)\n",
    "        \n",
    "        # sliding window optimization\n",
    "        \n",
    "        ys = [self.Ls[0](samples[0])]\n",
    "        for _ in range(len(samples)-1):\n",
    "            ys.append(self.Ls[_+1](torch.cat(samples[:_+2], dim =-1)))\n",
    "        \n",
    "        \n",
    "#         ys = [self.relu(y) for y in ys]\n",
    "        ys = [F.log_softmax(y, dim = -1) for y in ys]\n",
    "        return ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(880)\n",
    "model = Net()\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(92)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "I = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.413694\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.286838\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.232370\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.157154\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.127024\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.983122\n",
      "\n",
      "Test set: Average loss: 1.9580, Accuracy: 2966/10000 (30%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.912698\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.899830\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.880443\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.850075\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.693997\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.698250\n",
      "\n",
      "Test set: Average loss: 1.5924, Accuracy: 5316/10000 (53%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.622144\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.565860\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.489902\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.343526\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.404184\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.204426\n",
      "\n",
      "Test set: Average loss: 1.2185, Accuracy: 6789/10000 (68%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.176268\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.206413\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.279171\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.134438\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.073758\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.083165\n",
      "\n",
      "Test set: Average loss: 1.0314, Accuracy: 6741/10000 (67%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.981030\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.963716\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.948902\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.908177\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.922896\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.004992\n",
      "\n",
      "Test set: Average loss: 0.9310, Accuracy: 6844/10000 (68%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.923295\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.894508\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.971712\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.802449\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.850484\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.869407\n",
      "\n",
      "Test set: Average loss: 0.8279, Accuracy: 6932/10000 (69%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.760598\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.881238\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.790332\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.717529\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.648398\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.616063\n",
      "\n",
      "Test set: Average loss: 0.5774, Accuracy: 7882/10000 (79%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.580599\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.561524\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.554889\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.510564\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.616310\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.522669\n",
      "\n",
      "Test set: Average loss: 0.5086, Accuracy: 7884/10000 (79%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.491828\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.577615\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.490670\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.583212\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.408753\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.449647\n",
      "\n",
      "Test set: Average loss: 0.4458, Accuracy: 8651/10000 (87%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.487156\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.430223\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.376006\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.362553\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.408095\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.349301\n",
      "\n",
      "Test set: Average loss: 0.3667, Accuracy: 8827/10000 (88%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.415009\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 0.297506\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.291536\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 0.312645\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.364384\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 0.292894\n",
      "\n",
      "Test set: Average loss: 0.3285, Accuracy: 8829/10000 (88%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.367786\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 0.278756\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.295145\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 0.266405\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.240095\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 0.331266\n",
      "\n",
      "Test set: Average loss: 0.2970, Accuracy: 8945/10000 (89%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.321881\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 0.355071\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.307543\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 0.333000\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.185452\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 0.169657\n",
      "\n",
      "Test set: Average loss: 0.1994, Accuracy: 9806/10000 (98%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.147101\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 0.123417\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.101861\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 0.162663\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.237667\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 0.130947\n",
      "\n",
      "Test set: Average loss: 0.1594, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.162193\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 0.219805\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.148814\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 0.076361\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.075740\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 0.080330\n",
      "\n",
      "Test set: Average loss: 0.1490, Accuracy: 9834/10000 (98%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.073859\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 0.157557\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.171270\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 0.099198\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.140762\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 0.060210\n",
      "\n",
      "Test set: Average loss: 0.1254, Accuracy: 9867/10000 (99%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.105136\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 0.053579\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.093974\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 0.092527\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.151287\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 0.165779\n",
      "\n",
      "Test set: Average loss: 0.1330, Accuracy: 9838/10000 (98%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.198550\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 0.052613\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.114362\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 0.042173\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.040383\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 0.088407\n",
      "\n",
      "Test set: Average loss: 0.1254, Accuracy: 9854/10000 (99%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.037527\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 0.080564\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.037442\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 0.079808\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.132427\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 0.126253\n",
      "\n",
      "Test set: Average loss: 0.1198, Accuracy: 9844/10000 (98%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.033345\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 0.032406\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 0.130413\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 0.182014\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.030965\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 0.075436\n",
      "\n",
      "Test set: Average loss: 0.1238, Accuracy: 9845/10000 (98%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.079909\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 0.067813\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 0.028557\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 0.027441\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 0.041639\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 0.133992\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-186-af005ec0aa70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-186-af005ec0aa70>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mCUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2311\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2313\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2259\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m             im = im._new(\n\u001b[0;32m-> 2261\u001b[0;31m                 \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m                 )\n\u001b[1;32m   2263\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sliding window optimization\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data = data.view(data.shape[0], 28*28)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[I]\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data = data.view(data.shape[0], 28*28)\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        output = model(data)[I]\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 80 + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for data, target in test_loader:\n",
    "    s = Variable(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0542  0.0542 -0.1040  ...   0.1166  0.0321  0.0881\n",
       " 0.0722  0.1088 -0.1926  ...   0.1206 -0.0525  0.1383\n",
       " 0.0384  0.0524 -0.0770  ...   0.1045  0.0305  0.0386\n",
       "          ...             â‹±             ...          \n",
       " 0.0697  0.0397 -0.1250  ...   0.0965  0.0214  0.0888\n",
       " 0.0801  0.1002 -0.1658  ...   0.0643 -0.0488  0.1180\n",
       " 0.0713 -0.0030 -0.0844  ...   0.0354  0.0118  0.0241\n",
       "[torch.FloatTensor of size 100x128]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder()\n",
    "E(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.324016\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.181027\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.094387\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.961514\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.920716\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.864542\n",
      "\n",
      "Test set: Average loss: 1.7906, Accuracy: 4189/10000 (42%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.830534\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.731514\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.663590\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.684033\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.542865\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.504835\n",
      "\n",
      "Test set: Average loss: 1.5756, Accuracy: 4236/10000 (42%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.513642\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.471006\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.530206\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.537496\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.301418\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.396815\n",
      "\n",
      "Test set: Average loss: 1.4392, Accuracy: 4327/10000 (43%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.441318\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.465235\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.485566\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.344580\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.350666\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.271819\n",
      "\n",
      "Test set: Average loss: 1.2382, Accuracy: 5722/10000 (57%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.438831\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 1.142857\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.132250\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 1.180458\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.122613\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.252345\n",
      "\n",
      "Test set: Average loss: 1.1353, Accuracy: 5631/10000 (56%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.079492\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.166182\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.970726\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.852165\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.036630\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.258083\n",
      "\n",
      "Test set: Average loss: 1.0737, Accuracy: 5732/10000 (57%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.100012\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.092805\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.960027\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.915288\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.980091\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.064906\n",
      "\n",
      "Test set: Average loss: 1.0486, Accuracy: 5691/10000 (57%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.995952\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.109991\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.996315\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.081226\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.807286\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.778826\n",
      "\n",
      "Test set: Average loss: 1.0340, Accuracy: 5674/10000 (57%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.098839\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.807600\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.963633\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 1.060330\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.074862\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.809158\n",
      "\n",
      "Test set: Average loss: 1.0111, Accuracy: 5782/10000 (58%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.105764\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.969265\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.091409\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.146272\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.778772\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.889725\n",
      "\n",
      "Test set: Average loss: 0.9938, Accuracy: 5705/10000 (57%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.846231\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 1.067546\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.872621\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 0.994678\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.792733\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 1.039736\n",
      "\n",
      "Test set: Average loss: 1.0058, Accuracy: 5656/10000 (57%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.088752\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 0.942441\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.821410\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 1.133035\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.919264\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 1.000130\n",
      "\n",
      "Test set: Average loss: 1.0135, Accuracy: 5711/10000 (57%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.948753\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 0.932707\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.919859\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 1.051684\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 1.240429\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 0.942246\n",
      "\n",
      "Test set: Average loss: 1.1248, Accuracy: 6645/10000 (66%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.963757\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 0.890744\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 1.177751\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 1.058423\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.958407\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 0.996449\n",
      "\n",
      "Test set: Average loss: 1.0737, Accuracy: 6594/10000 (66%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.046001\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 1.191398\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 1.098787\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 1.035009\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.964334\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 1.201175\n",
      "\n",
      "Test set: Average loss: 1.0791, Accuracy: 6613/10000 (66%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.979973\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 0.902341\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.990376\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 1.390068\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 1.190206\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 1.039221\n",
      "\n",
      "Test set: Average loss: 1.0744, Accuracy: 6565/10000 (66%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.051629\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 1.226005\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.891104\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 0.918693\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 1.142699\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 1.077858\n",
      "\n",
      "Test set: Average loss: 1.0496, Accuracy: 6697/10000 (67%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.940526\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 1.232143\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.893339\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 1.112831\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 1.120989\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 1.006395\n",
      "\n",
      "Test set: Average loss: 1.0596, Accuracy: 6642/10000 (66%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.892494\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 0.964500\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.984600\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 1.005010\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.991252\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 1.027547\n",
      "\n",
      "Test set: Average loss: 1.0651, Accuracy: 6519/10000 (65%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 1.099357\n",
      "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 0.866730\n",
      "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 1.105605\n",
      "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 1.062534\n",
      "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 1.094602\n",
      "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 0.981169\n",
      "\n",
      "Test set: Average loss: 1.0415, Accuracy: 6671/10000 (67%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 1.073061\n",
      "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 1.210328\n",
      "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 1.174414\n",
      "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 0.934032\n",
      "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 0.946518\n",
      "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 0.888477\n",
      "\n",
      "Test set: Average loss: 1.0758, Accuracy: 6615/10000 (66%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 1.071859\n",
      "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 0.955742\n",
      "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 1.058534\n",
      "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 0.979907\n",
      "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 1.048808\n",
      "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 1.048363\n",
      "\n",
      "Test set: Average loss: 1.0927, Accuracy: 6553/10000 (66%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 1.245087\n",
      "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 0.958254\n",
      "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 0.944078\n",
      "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 1.003911\n",
      "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 0.975577\n",
      "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 1.043767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0493, Accuracy: 6617/10000 (66%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.915672\n",
      "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 1.026298\n",
      "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 0.990108\n",
      "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 0.783080\n",
      "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 0.900567\n",
      "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 1.066752\n",
      "\n",
      "Test set: Average loss: 1.0390, Accuracy: 6680/10000 (67%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 1.111115\n",
      "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 1.190912\n",
      "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 1.070360\n",
      "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 1.101280\n",
      "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 0.776606\n",
      "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 0.942170\n",
      "\n",
      "Test set: Average loss: 1.0702, Accuracy: 6576/10000 (66%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 1.095034\n",
      "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 0.978490\n",
      "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 1.070515\n",
      "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 0.966054\n",
      "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 1.306320\n",
      "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 1.315112\n",
      "\n",
      "Test set: Average loss: 1.0722, Accuracy: 6476/10000 (65%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 1.050029\n",
      "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 1.003438\n",
      "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 0.975980\n",
      "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 0.930846\n",
      "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 1.007766\n",
      "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 1.102248\n",
      "\n",
      "Test set: Average loss: 1.0748, Accuracy: 6511/10000 (65%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 1.014305\n",
      "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 0.925974\n",
      "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 1.120692\n",
      "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 1.070546\n",
      "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 1.212986\n",
      "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 0.837944\n",
      "\n",
      "Test set: Average loss: 1.0581, Accuracy: 6632/10000 (66%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 1.138731\n",
      "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 1.052567\n",
      "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 0.819412\n",
      "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 1.089641\n",
      "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 0.972795\n",
      "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 1.072043\n",
      "\n",
      "Test set: Average loss: 1.0921, Accuracy: 6429/10000 (64%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.804297\n",
      "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 0.929628\n",
      "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 1.066620\n",
      "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 1.159101\n",
      "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 0.918983\n",
      "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 1.139053\n",
      "\n",
      "Test set: Average loss: 1.1564, Accuracy: 6149/10000 (61%)\n",
      "\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 1.086561\n",
      "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 1.021065\n",
      "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 0.957225\n",
      "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 0.861340\n",
      "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 0.837644\n",
      "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 1.012586\n",
      "\n",
      "Test set: Average loss: 1.0789, Accuracy: 6519/10000 (65%)\n",
      "\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 1.093266\n",
      "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 1.173321\n",
      "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 1.086921\n",
      "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 0.991254\n",
      "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 0.952337\n",
      "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 1.046594\n",
      "\n",
      "Test set: Average loss: 1.0583, Accuracy: 6630/10000 (66%)\n",
      "\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 0.896096\n",
      "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 1.188191\n",
      "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 0.967047\n",
      "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 1.514147\n",
      "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 1.049156\n",
      "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 1.109367\n",
      "\n",
      "Test set: Average loss: 1.0590, Accuracy: 6571/10000 (66%)\n",
      "\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 0.929635\n",
      "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 0.984858\n",
      "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 1.018602\n",
      "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 1.306608\n",
      "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 0.984271\n",
      "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 1.064939\n",
      "\n",
      "Test set: Average loss: 1.0541, Accuracy: 6571/10000 (66%)\n",
      "\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 1.019904\n",
      "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 1.006564\n",
      "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 0.789682\n",
      "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 1.079512\n",
      "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 1.172375\n",
      "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 1.185956\n",
      "\n",
      "Test set: Average loss: 1.0524, Accuracy: 6624/10000 (66%)\n",
      "\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 1.019946\n",
      "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 0.989957\n",
      "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 1.189534\n",
      "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 0.839402\n",
      "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 1.118588\n",
      "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 0.974150\n",
      "\n",
      "Test set: Average loss: 1.0863, Accuracy: 6477/10000 (65%)\n",
      "\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 1.065296\n",
      "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 1.179785\n",
      "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 0.973243\n",
      "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 1.049342\n",
      "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 1.134254\n",
      "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 0.842726\n",
      "\n",
      "Test set: Average loss: 1.0619, Accuracy: 6590/10000 (66%)\n",
      "\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 0.808493\n",
      "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 0.996240\n",
      "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 1.071376\n",
      "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 1.019464\n",
      "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 0.862279\n",
      "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 0.973679\n",
      "\n",
      "Test set: Average loss: 1.0854, Accuracy: 6454/10000 (65%)\n",
      "\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 1.076493\n",
      "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 0.962023\n",
      "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 1.071353\n",
      "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 1.043116\n",
      "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 0.882566\n",
      "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 1.233869\n",
      "\n",
      "Test set: Average loss: 1.0923, Accuracy: 6421/10000 (64%)\n",
      "\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 1.001926\n",
      "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 1.260581\n",
      "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 1.124480\n",
      "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 0.896574\n",
      "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 1.150081\n",
      "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 0.770719\n",
      "\n",
      "Test set: Average loss: 1.0651, Accuracy: 6604/10000 (66%)\n",
      "\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 1.181729\n",
      "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 1.088265\n",
      "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 0.980356\n",
      "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 0.815160\n",
      "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 1.042122\n",
      "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 1.128405\n",
      "\n",
      "Test set: Average loss: 1.0689, Accuracy: 6570/10000 (66%)\n",
      "\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 1.026329\n",
      "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 0.849596\n",
      "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 0.795612\n",
      "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 0.808877\n",
      "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 0.995992\n",
      "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 0.836074\n",
      "\n",
      "Test set: Average loss: 1.0563, Accuracy: 6622/10000 (66%)\n",
      "\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 0.972173\n",
      "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 0.768032\n",
      "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 1.158458\n",
      "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 1.041936\n",
      "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 1.169553\n",
      "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 1.065063\n",
      "\n",
      "Test set: Average loss: 1.0874, Accuracy: 6513/10000 (65%)\n",
      "\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 1.111805\n",
      "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 1.087847\n",
      "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 0.967557\n",
      "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 1.140300\n",
      "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 0.934696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d5cc18df0ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-d5cc18df0ae7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mCUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(sequence, dim, out)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/weizhen/anaconda3/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data = data.view(data.shape[0], 28*28)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data = data.view(data.shape[0], 28*28)\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        \n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 80 + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(32)\n",
    "for epoch in range(30):\n",
    "    q, bits = train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(32)\n",
    "for epoch in range(10):\n",
    "    q, bits = train(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
