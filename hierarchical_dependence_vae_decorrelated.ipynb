{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        lambda x: x>0.5,\n",
    "        lambda x: x.float(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loading\n",
    "bsize = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=True, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dset.MNIST('data', train=False, download=True, transform=data_transforms), batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "N = 5\n",
    "k = 2\n",
    "tau = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UPDATE TO BINARY GUMBEL (REFER TO L0-REG PAPER)\n",
    "# gumbel-softmax\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    v = Variable(torch.log(-torch.log(U + eps) + eps))\n",
    "    if CUDA:\n",
    "        v = v.cuda()\n",
    "    return -v\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    return (y_hard - y).detach() + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        l = self.relu(self.fc2(h1))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tunnel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tunnel, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.relu(self.fc1(x))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Output, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        l = h1.view(-1,2)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Gate, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.sigmoid(self.fc1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(outputs):\n",
    "    outputs = [F.softmax(out, dim=-1) for out in outputs]\n",
    "    return torch.cat(outputs, dim=1).view(-1, N, k)\n",
    "\n",
    "def sample(l):\n",
    "    gs = gumbel_softmax(l, tau)\n",
    "    return gs.narrow(1,0,1)\n",
    "\n",
    "# def signal(outputs):\n",
    "#     outputs = [sample(out).narrow(1,0,1) for out in outputs]\n",
    "#     return torch.cat(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecDecoderHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecDecoderHead, self).__init__()\n",
    "        self.fc = nn.Linear(1, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc(x))\n",
    "    \n",
    "class RecDecoderBody(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecDecoderBody, self).__init__()\n",
    "        self.fc = nn.Linear(129, 128) # prev state + hidden\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc(x))\n",
    "    \n",
    "class RecDecoderTail(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(RecDecoderTail, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)      \n",
    "        self.fc3 = nn.Linear(512, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        h3 = self.fc3(h2)\n",
    "        o = h3.view(-1,784)\n",
    "        return F.sigmoid(o)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = Encoder()\n",
    "O0 = Output()\n",
    "Os = [Output() for _ in range(N-1)]\n",
    "Ts = [Tunnel() for _ in range(N-1)]\n",
    "T1s = [Tunnel() for _ in range(N-1)]\n",
    "Gs = [Gate() for _ in range(N-1)]\n",
    "D_head = RecDecoderHead()\n",
    "D_body = [RecDecoderBody() for _ in range(N-1)]\n",
    "D_tail = RecDecoderTail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    E.cuda()\n",
    "    O0.cuda()\n",
    "    [e.cuda() for e in Os]\n",
    "    [e.cuda() for e in Ts]\n",
    "    [e.cuda() for e in T1s]\n",
    "    [e.cuda() for e in Gs]\n",
    "    D_head.cuda()\n",
    "    [e.cuda() for e in D_body]\n",
    "    D_tail.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('mkdir -p checkpoint_recdecoder')\n",
    "PATH = 'checkpoint_recdecoder/hd_vae'\n",
    "def save_model():\n",
    "    torch.save(E.state_dict(), \"{}_E\".format(PATH))\n",
    "    torch.save(O0.state_dict(), \"{}_O0\".format(PATH))\n",
    "    \n",
    "    def save_list(models, name):\n",
    "        for i in range(len(models)):\n",
    "            torch.save(models[i].state_dict(), \"{}_{}_{}\".format(PATH, name, i))\n",
    "    \n",
    "    save_list(Os, 'Os')\n",
    "    save_list(Ts, 'Ts')\n",
    "    save_list(T1s, 'T1s')\n",
    "    save_list(Gs, 'Gs')\n",
    "    \n",
    "    torch.save(D_head.state_dict(), \"{}_D_head\".format(PATH))\n",
    "    save_list(D_body, 'D_body')\n",
    "    torch.save(D_tail.state_dict(), \"{}_D_tail\".format(PATH))\n",
    "    \n",
    "#save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "    \n",
    "log_two_pow_n = Variable(torch.Tensor([2**N]).log(), requires_grad=False)\n",
    "if CUDA:\n",
    "    log_two_pow_n = log_two_pow_n.cuda()\n",
    "\n",
    "def get_dependent_prior_loss(x):\n",
    "    acc = Variable(torch.zeros(2**N))\n",
    "    if CUDA:\n",
    "        acc = acc.cuda()\n",
    "    acc_i = 0\n",
    "    for _path in product(range(2), repeat=N):\n",
    "        path = Variable(torch.Tensor(_path), requires_grad=False)\n",
    "        if CUDA:\n",
    "            path = path.cuda()\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "        decoder_hidden = D_head(path[0])\n",
    "\n",
    "        for n in range(len(Ts)-1):\n",
    "            sofar = Ts[n](sofar)*Gs[n](decoder_hidden) # Kind of doing \"teacher forcing\"\n",
    "            outputs.append(Os[n](sofar))\n",
    "            decoder_hidden_inp = torch.cat([decoder_hidden, path[n]], -1)\n",
    "            decoder_hidden = D_body[n](decoder_hidden_inp)\n",
    "        \n",
    "        sofar = Ts[-1](sofar)*Gs[-1](decoder_hidden) # Kind of doing \"teacher forcing\"\n",
    "        outputs.append(Os[-1](sofar))\n",
    "        \n",
    "        q = prob(outputs)\n",
    "\n",
    "        _idx = path.data\n",
    "        select_mat = Variable(torch.stack([1-_idx, _idx]).t(), requires_grad=False)\n",
    "        if CUDA:\n",
    "            select_mat = select_mat.cuda()\n",
    "        select_mat = select_mat.expand(torch.Size([x.shape[0]]) + select_mat.shape)\n",
    "        probs = q.mul(select_mat).sum(-1) # shape (batch x N)\n",
    "        log_q = (probs+1e-20).log().sum(-1) # shape (batch)\n",
    "        kl_term = log_q.exp().mul(log_two_pow_n + log_q) # shape (batch)\n",
    "        kl_term = kl_term.mean() # shape (1) - take sum over minibatch\n",
    "\n",
    "        acc[acc_i] = kl_term\n",
    "        acc_i += 1\n",
    "\n",
    "    return acc.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "# Hacky! Getting a data point.\n",
    "for batch_idx, (data, _) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    if CUDA:\n",
    "        data = data.cuda()\n",
    "    x = data.view(-1,784)\n",
    "    x = x[0:4]\n",
    "    break\n",
    "    \n",
    "get_dependent_prior_loss(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# objective\n",
    "def loss_function(recon_x, x, q):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)/bsize # TODO eddie: changed from size_average=False\n",
    "    # Assuming independent latent\n",
    "#     KLD = torch.sum(q*(torch.log(q+1E-20)-np.log(1/k)))\n",
    "\n",
    "    # Assuming dependent latent\n",
    "    KLD = get_dependent_prior_loss(x)\n",
    "    return BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    {'params':E.parameters()},\n",
    "    {'params':O0.parameters()} \n",
    "] + [{'params':o.parameters()} for o in Os] \\\n",
    "  + [{'params':t.parameters()} for t in Ts] \\\n",
    "  + [{'params': g.parameters()} for g in Gs] \\\n",
    "  + [{'params': D_head.parameters()}, {'params': D_tail.parameters()}] \\\n",
    "  + [{'params': d_body.parameters()} for d_body in D_body]\n",
    "optimizer = optim.Adam(params, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss_sofar = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def debug():\n",
    "    f, axes = plt.subplots(1, 2**N, sharey=True, figsize=(25,1))\n",
    "    kk = 0\n",
    "\n",
    "    def make_variable(value):\n",
    "        t = torch.Tensor([value]).type(torch.FloatTensor)\n",
    "        t = t.unsqueeze(0)\n",
    "        v = Variable(t)\n",
    "        if CUDA:\n",
    "            v = v.cuda()\n",
    "        return v\n",
    "\n",
    "    for path in product(range(2), repeat=N):\n",
    "        decoder_hidden = D_head(make_variable(path[0]))\n",
    "        for n in range(1, len(path)):\n",
    "            decoder_hidden_inp = torch.cat([decoder_hidden, make_variable(path[n])], -1)\n",
    "            decoder_hidden = D_body[n-1](decoder_hidden_inp)\n",
    "        w = D_tail(decoder_hidden)\n",
    "\n",
    "        ax = axes[kk]\n",
    "        ax.set_title(path)\n",
    "        ax.imshow(w.view(28,28).cpu().data.numpy())\n",
    "        kk += 1\n",
    "    plt.show()\n",
    "    \n",
    "debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    global best_loss_sofar\n",
    "    train_bce_loss = 0\n",
    "    train_kld_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        x = data.view(-1,784)\n",
    "        optimizer.zero_grad()\n",
    "        sofar = E(x)\n",
    "        outputs = [O0(sofar)]\n",
    "        samples = [sample(outputs[-1])]\n",
    "        decoder_hidden = D_head(samples[-1])\n",
    "        # TODO unroll this may make it faster\n",
    "        for n in range(len(Ts)):\n",
    "            sofar = Ts[n](sofar)*Gs[n](decoder_hidden)\n",
    "            outputs.append(Os[n](sofar))\n",
    "            samples.append(sample(outputs[-1]))\n",
    "            \n",
    "            decoder_hidden_inp = torch.cat([decoder_hidden, samples[-1]], -1)\n",
    "            decoder_hidden = D_body[n](decoder_hidden_inp)\n",
    "            \n",
    "        recon_x = D_tail(decoder_hidden)\n",
    "     \n",
    "        bits = torch.cat(samples, dim=1) # for debugging only\n",
    "        q = prob(outputs)\n",
    "\n",
    "        BCE, KLD = loss_function(recon_x, x, q)\n",
    "        loss = BCE + KLD\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_bce_loss += BCE.data[0]\n",
    "        train_kld_loss += KLD.data[0]\n",
    "        \n",
    "#         if batch_idx % 200 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBCE loss: {:.6f}\\tKLD loss: {:.6f}'.format(\n",
    "#                 epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader),\n",
    "#                 BCE.data[0] / len(data),\n",
    "#                 KLD.data[0] / len(data)))\n",
    "            \n",
    "    avg_bce_loss = train_bce_loss / len(train_loader.dataset)\n",
    "    avg_kld_loss = train_kld_loss / len(train_loader.dataset)\n",
    "    avg_loss = avg_bce_loss + avg_kld_loss\n",
    "    print('====> Epoch: {} Average BCE loss: {:.4f}, Average KLD loss: {:.4f}, Total: {:.4f}'.format(\n",
    "          epoch+1, avg_bce_loss, avg_kld_loss, avg_loss))\n",
    "    \n",
    "    if avg_loss < best_loss_sofar:\n",
    "        print(\"Loss {} is better than previous best {}, saving model\".format(avg_loss, best_loss_sofar))\n",
    "        save_model()\n",
    "        best_loss_sofar = avg_loss\n",
    "        debug()\n",
    "        \n",
    "    return q, bits # debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(1):\n",
    "    q, bits = train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
